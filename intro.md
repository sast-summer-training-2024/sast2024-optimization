# 机器学习中的优化方法

​    如果用几个字概括 AI 模型的训练过程，大概就是“最小化损失函数”。在高维空间中，寻找一个函数的最小值点（甚至只是局部最小值）并非易事，而“优化”正是研究这类问题的数学分支。我们都已熟悉反向传播和梯度下降法，也亲眼见证了这样简单的优化方法所赋予模型的神奇魔力。但是，是否有可能从理论的角度给出某类优化算法的复杂度保证呢？ 

​    在本课程中，我们将： 

1. 介绍机器学习中常用的优化方法，包括梯度下降法、随机梯度下降法、条件梯度法等； 

2. 在一定的假设下（如损失函数满足凸性/非凸/光滑/利普希茨连续等），研究优化算法收敛性与复杂度的理论保证；

3. 简要介绍复杂度下界的证明方法； 
4. 如有可能，还将简要介绍一些 advanced topics，如方差缩减技术、Nesterov 加速梯度下降法等。 

​    希望同学们能通过本课程了解一些经典的一阶优化算法及复杂度分析方法，并能对优化这一领域建立一些最基本的认识。本课程应基本不会涉及代码编写。如果愿意的话，可以在身旁准备一支铅笔和几张草稿纸~

## 课前准备

本课程不需要进行太多额外的课前准备。感兴趣的同学可先阅读讲义的“预备知识”部分。

## 课程讲义

[看这里](https://github.com/sast-summer-training-2024/sast2024-optimization/blob/master/slides%20(draft%20version).pdf)